{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in c:\\users\\dvtha\\appdata\\roaming\\python\\python311\\site-packages (3.5.1)\n",
      "Requirement already satisfied: findspark in c:\\users\\dvtha\\appdata\\roaming\\python\\python311\\site-packages (2.0.1)\n",
      "Requirement already satisfied: wget in c:\\users\\dvtha\\appdata\\roaming\\python\\python311\\site-packages (3.2)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\dvtha\\appdata\\roaming\\python\\python311\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "# Installing required packages  \n",
    "!pip install pyspark  findspark wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PySpark is the Spark API for Python. In this lab, we use PySpark to initialize the SparkContext.   \n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import col, sum, date_format, to_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a SparkContext object\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Creating a SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"test_spark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_schema = StructType([\n",
    "    StructField(\"code\", IntegerType(), True),\n",
    "    StructField(\"fullname\", StringType(), True)\n",
    "])\n",
    "\n",
    "activity_schema = StructType([\n",
    "    StructField(\"student_code\", IntegerType(), True),\n",
    "    StructField(\"activity\", StringType(), True),\n",
    "    StructField(\"numberOfFile\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'danh_sach_sv_de.csv'\n",
    "parquet_file_path = 'log_action.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV preview:\n",
      "root\n",
      " |-- code: integer (nullable = true)\n",
      " |-- fullname: string (nullable = true)\n",
      "\n",
      "+----+-----------------+\n",
      "|code|         fullname|\n",
      "+----+-----------------+\n",
      "|   1|       Mai Đức An|\n",
      "|   2|   Nguyễn Mai Anh|\n",
      "|   3|Ngô Ngọc Tuấn Anh|\n",
      "|   4|   Trần Trung Anh|\n",
      "|   5|    Trần Ngọc Bảo|\n",
      "+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student_df = spark.read.csv(csv_file_path, header=False, schema=student_schema)\n",
    "\n",
    "print(\"CSV preview:\")\n",
    "student_df.printSchema()\n",
    "student_df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet Schema:\n",
      "root\n",
      " |-- student_code: integer (nullable = true)\n",
      " |-- activity: string (nullable = true)\n",
      " |-- numberOfFile: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "+------------+--------+------------+---------+\n",
      "|student_code|activity|numberOfFile|timestamp|\n",
      "+------------+--------+------------+---------+\n",
      "|           4|   write|           7|6/10/2024|\n",
      "|          33|    read|           5|6/12/2024|\n",
      "|          33| execute|           1|6/13/2024|\n",
      "|           6|   write|           6|6/15/2024|\n",
      "|          24| execute|           8|6/12/2024|\n",
      "+------------+--------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity_df = spark.read.csv(parquet_file_path, header=False, schema=activity_schema)\n",
    "\n",
    "print(\"Parquet Schema:\")\n",
    "activity_df.printSchema()\n",
    "activity_df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+------------+---------+----+-----------------+\n",
      "|student_code|activity|numberOfFile|timestamp|code|         fullname|\n",
      "+------------+--------+------------+---------+----+-----------------+\n",
      "|           1|    read|           5|6/13/2024|   1|       Mai Đức An|\n",
      "|           1|    read|           8|6/10/2024|   1|       Mai Đức An|\n",
      "|           1|    read|           9|6/13/2024|   1|       Mai Đức An|\n",
      "|           1|    read|           5|6/11/2024|   1|       Mai Đức An|\n",
      "|           1|    read|           6|6/11/2024|   1|       Mai Đức An|\n",
      "|           1|    read|           9|6/10/2024|   1|       Mai Đức An|\n",
      "|           1|   write|           4|6/14/2024|   1|       Mai Đức An|\n",
      "|           1| execute|           3|6/11/2024|   1|       Mai Đức An|\n",
      "|           1| execute|          10|6/12/2024|   1|       Mai Đức An|\n",
      "|           1|    read|           8|6/10/2024|   1|       Mai Đức An|\n",
      "|           1|   write|           6|6/10/2024|   1|       Mai Đức An|\n",
      "|           1|   write|          10|6/13/2024|   1|       Mai Đức An|\n",
      "|           1|    read|           7|6/15/2024|   1|       Mai Đức An|\n",
      "|           2|   write|          10|6/12/2024|   2|   Nguyễn Mai Anh|\n",
      "|           2|   write|           2|6/15/2024|   2|   Nguyễn Mai Anh|\n",
      "|           2|   write|           9|6/12/2024|   2|   Nguyễn Mai Anh|\n",
      "|           2| execute|           1|6/12/2024|   2|   Nguyễn Mai Anh|\n",
      "|           2|    read|           3|6/13/2024|   2|   Nguyễn Mai Anh|\n",
      "|           2|   write|           1|6/11/2024|   2|   Nguyễn Mai Anh|\n",
      "|           3|    read|           9|6/13/2024|   3|Ngô Ngọc Tuấn Anh|\n",
      "+------------+--------+------------+---------+----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = activity_df.join(student_df, activity_df['student_code'] == student_df['code'], 'outer')\n",
    "\n",
    "raw_df.show()\n",
    "raw_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records:  400\n"
     ]
    }
   ],
   "source": [
    "transformed_df = raw_df.withColumn('date', date_format(to_date(col('timestamp'), 'M/dd/yyyy'), 'yyyyMMdd'))\n",
    "transformed_df = transformed_df.withColumnRenamed(\"fullname\", \"student_name\")\n",
    "print(\"Number of records: \", transformed_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------------------+--------+---------+\n",
      "|    date|student_code|        student_name|activity|totalFile|\n",
      "+--------+------------+--------------------+--------+---------+\n",
      "|20240612|          10|    Nguyễn Quốc Dũng|    read|        8|\n",
      "|20240614|          16|        Đỗ Doãn Khắc| execute|        9|\n",
      "|20240612|          17|      Châu Minh Khải|   write|        3|\n",
      "|20240615|           3|   Ngô Ngọc Tuấn Anh|    read|       24|\n",
      "|20240612|           8|        Đỗ Thành Đạt|    read|        3|\n",
      "|20240614|          19|        Lê Bảo Khánh|    read|        1|\n",
      "|20240611|          20|        Lê Minh Phúc|    read|        6|\n",
      "|20240613|          37|          Đào Anh Vũ| execute|        1|\n",
      "|20240611|          14|        Ngô Phi Hùng| execute|       12|\n",
      "|20240611|          21| Trần Phúc Mạnh Linh| execute|       10|\n",
      "|20240614|          36|        Vũ Khắc Long|    read|        5|\n",
      "|20240611|          36|        Vũ Khắc Long| execute|        3|\n",
      "|20240614|           5|       Trần Ngọc Bảo|    read|        1|\n",
      "|20240614|           8|        Đỗ Thành Đạt| execute|        6|\n",
      "|20240613|          13|    Nguyễn Minh Hiếu|   write|        8|\n",
      "|20240612|           5|       Trần Ngọc Bảo|    read|        6|\n",
      "|20240613|          25|    Đào Thanh Nguyên|   write|        8|\n",
      "|20240611|           4|      Trần Trung Anh| execute|        7|\n",
      "|20240614|          32|     Đinh Việt Thành|    read|        7|\n",
      "|20240610|          28|Lương Thị Mai Phương| execute|        1|\n",
      "+--------+------------+--------------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = [col('date'), col('student_code'), col('student_name'), col('activity')]\n",
    "condition = (col(\"activity\").isin([\"Football\", \"Basketball\", \"Tennis\"]))\n",
    "\n",
    "\n",
    "transformed_df.createOrReplaceTempView(\"temp_view\")\n",
    "query = \"\"\"\n",
    "    SELECT date, student_code, student_name, activity, int(sum(numberOfFile)) as totalFile\n",
    "    FROM temp_view\n",
    "    GROUP BY date, student_code, student_name, activity\n",
    "\"\"\"\n",
    "# query = \"\"\"\n",
    "#     select date, student_code\n",
    "#     from temp_view\n",
    "# \"\"\"\n",
    "\n",
    "# # Execute the SQL query\n",
    "result_df = spark.sql(query)\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# output_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
